{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/Large-Language-Model/blob/main/transformer_from_complete_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-L1VFg1zmFu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import regex as re\n",
        "\n",
        "class Tokenization:\n",
        "  def __init__(self):\n",
        "    self.vocab_size = 0\n",
        "    self.verbose = False\n",
        "    self.merges = {} # (int,int) -> (int)\n",
        "    self.vocab = {} # Maps token ID â†’ bytes/string\n",
        "    self.vocab_inv = {} # Maps bytes/string â†’ token ID\n",
        "    self.re_pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\" , flags=re.IGNORECASE)\n",
        "\n",
        "  def pair_freq(self , tokens):\n",
        "    freq = {}\n",
        "    for pair in zip(tokens , tokens[1:]):\n",
        "      freq[pair] = freq.get(pair , 0) + 1\n",
        "    return freq\n",
        "\n",
        "  def merge_pairs(self , tokens,max_pair , idx_replaces_with):\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "      if i < len(tokens)-1 and tokens[i] == max_pair[0] and tokens[i+1] == max_pair[1]:\n",
        "        new_tokens.append(idx_replaces_with)\n",
        "        i += 2\n",
        "      else:\n",
        "        new_tokens.append(tokens[i])\n",
        "        i += 1\n",
        "    return new_tokens\n",
        "\n",
        "\n",
        "  def train(self , text , vocab_size , verbose=False):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.verbose = verbose\n",
        "    words = re.findall(self.re_pattern , text)\n",
        "    tokens = list(''.join(words).encode(\"utf-8\"))\n",
        "\n",
        "\n",
        "    # training loop\n",
        "    num_merges = self.vocab_size - 256\n",
        "    nxt_idx = 256\n",
        "    self.vocab = {idx : bytes([idx]) for idx in range(nxt_idx)}\n",
        "\n",
        "    for i in range(num_merges):\n",
        "      stats = self.pair_freq(tokens)\n",
        "      if not stats:\n",
        "        break\n",
        "      max_pair = max(stats , key=stats.get)\n",
        "      nxt_idx += 1\n",
        "      self.merges[max_pair] = nxt_idx\n",
        "\n",
        "      tokens = self.merge_pairs(tokens , max_pair , nxt_idx)\n",
        "\n",
        "      if verbose:\n",
        "        print(f\"merging {max_pair} --> {nxt_idx}\")\n",
        "\n",
        "      self.vocab[nxt_idx] = self.vocab[max_pair[0]] + self.vocab[max_pair[1]]\n",
        "    self.vocab_inv = {v:k for k,v in self.vocab.items()}\n",
        "\n",
        "  def encode(self , text):\n",
        "    words = re.findall(self.re_pattern , text)\n",
        "    tokens = list(''.join(words).encode(\"utf-8\"))\n",
        "\n",
        "    while True:\n",
        "      stats = self.pair_freq(tokens)\n",
        "      if not stats:\n",
        "        break\n",
        "\n",
        "      \"\"\"From all pairs in stats, select the one with lowest merge index (i.e., which was merged earliest during training).\"\"\"\n",
        "      pair = min(stats , key=lambda p : self.merges.get(p , float(\"inf\")))\n",
        "      if pair not in self.merges:\n",
        "        break\n",
        "\n",
        "      idx = self.merges[pair]\n",
        "      tokens = self.merge_pairs(tokens , pair , idx)\n",
        "    return tokens\n",
        "\n",
        "  def decode(self , tokens):\n",
        "    return b\"\".join(self.vocab[token] for token in tokens).decode(\"utf-8\" , errors=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5aqltWdaDdft",
        "outputId": "fe28d672-ede4-4158-c7b1-ea1e9f522337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-13 04:36:40--  https://raw.githubusercontent.com/karpathy/minbpe/refs/heads/master/tests/taylorswift.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185768 (181K) [text/plain]\n",
            "Saving to: â€˜taylorswift.txtâ€™\n",
            "\n",
            "taylorswift.txt     100%[===================>] 181.41K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-13 04:36:40 (12.3 MB/s) - â€˜taylorswift.txtâ€™ saved [185768/185768]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://raw.githubusercontent.com/karpathy/minbpe/refs/heads/master/tests/taylorswift.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQR-EFibDLrX"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenization()\n",
        "text = open(\"taylorswift.txt\" , \"r\").read()\n",
        "\n",
        "tokenizer.train(text , 50257)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORKIyR0HDwaX",
        "outputId": "a9e7d4ab-cec1-46d7-dd92-b8ffa31fb2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[104, 532, 370, 1449, 33, 33, 33, 63, 401, 236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 33, 896, 108, 408, 49, 424, 32, 240, 159, 152, 137]\n",
            "hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\n"
          ]
        }
      ],
      "source": [
        "# testing it\n",
        "\n",
        "tokens = tokenizer.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
        "print(tokens)\n",
        "\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "w9WBsYmMCt4t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True)\n",
        "        self.ln1 = nn.LayerNorm(emb_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 4 * emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * emb_dim, emb_dim)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(emb_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=mask)\n",
        "        x = self.ln1(x + attn_output)\n",
        "        x = self.ln2(x + self.ff(x))\n",
        "        return x\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=128, num_heads=4, num_layers=4, block_size=128):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(block_size, emb_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SelfAttentionBlock(emb_dim, num_heads)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(emb_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
        "        self.block_size = block_size\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        tok_emb = self.token_embedding(x)\n",
        "        pos = torch.arange(0, T, device=x.device)\n",
        "        pos_emb = self.pos_embedding(pos)[None, :, :]\n",
        "\n",
        "        x = tok_emb + pos_emb  # (B, T, emb_dim)\n",
        "\n",
        "        # causal mask: (T, T) â†’ upper triangular with -inf\n",
        "        mask = torch.triu(torch.full((T, T), float('-inf'), device=x.device), diagonal=1)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.fc_out(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swEhULdkCx3N"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset , DataLoader\n",
        "\n",
        "class BPEDatasetLM(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_len=32):\n",
        "        self.seq_len = seq_len\n",
        "        tokens = []\n",
        "        for text in texts:\n",
        "            tokens.extend(tokenizer.encode(text))\n",
        "        self.data = []\n",
        "        for i in range(0, len(tokens) - seq_len):\n",
        "            x = tokens[i:i + seq_len]\n",
        "            y = tokens[i + 1:i + 1 + seq_len]\n",
        "            self.data.append((x, y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.data[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "dataset = BPEDatasetLM(text, tokenizer, seq_len=32)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSY5ne1HHZi7",
        "outputId": "645538be-b356-4b4d-dcd3-eb5950872f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32])\n",
            "torch.Size([32, 32])\n"
          ]
        }
      ],
      "source": [
        "x, y = next(iter(dataloader))\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO5vgyEykt0J",
        "outputId": "a09e86aa-0b3b-48f8-a1b2-cdf97791a454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HJOR4GVQFYX1"
      },
      "outputs": [],
      "source": [
        "model = TransformerLM(vocab_size=tokenizer.vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters() , lr=3e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vTYWlxxDB6gn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in dataloader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IdAqrBHrMth",
        "outputId": "b5eaf62c-0c69-444a-9f00-e5f4317d5b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32, 50257])\n"
          ]
        }
      ],
      "source": [
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G06oU46HKdKG"
      },
      "outputs": [],
      "source": [
        "def sample(model, tokenizer, prompt, max_new_tokens=100, temperature=0.8, top_k=40, device='cuda'):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    x = torch.tensor(tokens, dtype=torch.long, device=device)[None, :]  # (1, T)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        if x.size(1) > model.block_size:\n",
        "            x = x[:, -model.block_size:]\n",
        "\n",
        "        logits = model(x)[:, -1, :] / temperature  # (1, vocab)\n",
        "        if top_k is not None:\n",
        "            values, _ = torch.topk(logits, top_k)\n",
        "            logits[logits < values[:, -1, None]] = -float('inf')\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        x = torch.cat([x, next_token], dim=1)\n",
        "\n",
        "    return tokenizer.decode(x[0].tolist())\n",
        "\n",
        "print(sample(model, tokenizer, \"hello\" , 50))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMO6ARh01RQHtQzhc/nwM8A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}