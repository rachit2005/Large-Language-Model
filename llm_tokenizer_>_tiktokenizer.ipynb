{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbC4ejByKFMBMAXoVODhh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/Large-Language-Model/blob/main/llm_tokenizer_%3E_tiktokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QGdLzh34PX8",
        "outputId": "b0ffc597-e2cb-43c0-b06c-57e52321258d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# using unicode byte encoding in utf-8\n",
        "\n",
        "list(\"hello world\".encode(\"utf-8\")) # --> these are byte stream"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder (Vocabulary Construction):\n",
        "1. Initialization:\n",
        "The encoder begins by initializing the vocabulary with individual characters (or bytes) of the text.\n",
        "2. Iterative Merging:\n",
        "The encoder then iteratively merges the most frequent pairs of characters (or subwords) in the vocabulary.\n",
        "3. Vocabulary Expansion:\n",
        "This merging process expands the vocabulary with new subword units until a desired vocabulary size is reached.\n",
        "4. Vocabulary Storage:\n",
        "The final vocabulary of subwords, along with their corresponding merge rules, is stored for later use in the decoder.  "
      ],
      "metadata": {
        "id": "DPNTvAGTOh5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# byte pair encoding --> https://en.wikipedia.org/wiki/Byte-pair_encoding\n",
        "\n",
        "\n",
        "# 1. Initialization\n",
        "\n",
        "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\n",
        "tokens = text.encode(\"utf-8\")\n",
        "\n",
        "tokens = list(map(int , tokens))\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "print(len(text))\n",
        "print(len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gu1ajbDE2mw",
        "outputId": "10677b03-04d0-494d-8d16-da83c120804b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 32, 105, 115, 32, 115, 105, 109, 112, 108, 121, 32, 100, 117, 109, 109, 121, 32, 116, 101, 120, 116, 32, 111, 102, 32, 116, 104, 101, 32, 112, 114, 105, 110, 116, 105, 110, 103, 32, 97, 110, 100, 32, 116, 121, 112, 101, 115, 101, 116, 116, 105, 110, 103, 32, 105, 110, 100, 117, 115, 116, 114, 121, 46, 32, 76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 32, 104, 97, 115, 32, 98, 101, 101, 110, 32, 116, 104, 101, 32, 105, 110, 100, 117, 115, 116, 114, 121, 39, 115, 32, 115, 116, 97, 110, 100, 97, 114, 100, 32, 100, 117, 109, 109, 121, 32, 116, 101, 120, 116, 32, 101, 118, 101, 114, 32, 115, 105, 110, 99, 101, 32, 116, 104, 101, 32, 49, 53, 48, 48, 115, 44, 32, 119, 104, 101, 110, 32, 97, 110, 32, 117, 110, 107, 110, 111, 119, 110, 32, 112, 114, 105, 110, 116, 101, 114, 32, 116, 111, 111, 107, 32, 97, 32, 103, 97, 108, 108, 101, 121, 32, 111, 102, 32, 116, 121, 112, 101, 32, 97, 110, 100, 32, 115, 99, 114, 97, 109, 98, 108, 101, 100, 32, 105, 116, 32, 116, 111, 32, 109, 97, 107, 101, 32, 97, 32, 116, 121, 112, 101, 32, 115, 112, 101, 99, 105, 109, 101, 110, 32, 98, 111, 111, 107, 46, 32, 73, 116, 32, 104, 97, 115, 32, 115, 117, 114, 118, 105, 118, 101, 100, 32, 110, 111, 116, 32, 111, 110, 108, 121, 32, 102, 105, 118, 101, 32, 99, 101, 110, 116, 117, 114, 105, 101, 115, 44, 32, 98, 117, 116, 32, 97, 108, 115, 111, 32, 116, 104, 101, 32, 108, 101, 97, 112, 32, 105, 110, 116, 111, 32, 101, 108, 101, 99, 116, 114, 111, 110, 105, 99, 32, 116, 121, 112, 101, 115, 101, 116, 116, 105, 110, 103, 44, 32, 114, 101, 109, 97, 105, 110, 105, 110, 103, 32, 101, 115, 115, 101, 110, 116, 105, 97, 108, 108, 121, 32, 117, 110, 99, 104, 97, 110, 103, 101, 100, 46, 32, 73, 116, 32, 119, 97, 115, 32, 112, 111, 112, 117, 108, 97, 114, 105, 115, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 49, 57, 54, 48, 115, 32, 119, 105, 116, 104, 32, 116, 104, 101, 32, 114, 101, 108, 101, 97, 115, 101, 32, 111, 102, 32, 76, 101, 116, 114, 97, 115, 101, 116, 32, 115, 104, 101, 101, 116, 115, 32, 99, 111, 110, 116, 97, 105, 110, 105, 110, 103, 32, 76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 32, 112, 97, 115, 115, 97, 103, 101, 115, 44, 32, 97, 110, 100, 32, 109, 111, 114, 101, 32, 114, 101, 99, 101, 110, 116, 108, 121, 32, 119, 105, 116, 104, 32, 100, 101, 115, 107, 116, 111, 112, 32, 112, 117, 98, 108, 105, 115, 104, 105, 110, 103, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 108, 105, 107, 101, 32, 65, 108, 100, 117, 115, 32, 80, 97, 103, 101, 77, 97, 107, 101, 114, 32, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 118, 101, 114, 115, 105, 111, 110, 115, 32, 111, 102, 32, 76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 46]\n",
            "574\n",
            "574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the pair of bytes that occur most frequently\n",
        "\n",
        "def get_pair_freq(tokens):\n",
        "  pair_freq = {}\n",
        "  for pair in zip(tokens, tokens[1:]):\n",
        "    pair_freq[pair] = pair_freq.get(pair , 0) + 1\n",
        "  return pair_freq\n",
        "\n",
        "stats = get_pair_freq(tokens)\n",
        "print(sorted(((v,k) for k,v in stats.items()) , reverse=True))\n",
        "top_pair = max(stats , key = stats.get)\n",
        "print(top_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgNC7NoNG46u",
        "outputId": "b39671de-c052-4924-b149-ff0be29ccfb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(17, (105, 110)), (15, (101, 32)), (14, (32, 116)), (9, (115, 32)), (9, (114, 101)), (8, (116, 104)), (8, (116, 32)), (8, (110, 103)), (8, (104, 101)), (8, (32, 115)), (7, (121, 32)), (7, (110, 116)), (7, (109, 32)), (7, (100, 32)), (7, (32, 105)), (7, (32, 97)), (6, (117, 109)), (6, (115, 101)), (6, (110, 100)), (6, (110, 32)), (6, (103, 32)), (6, (101, 115)), (6, (101, 110)), (6, (97, 115)), (6, (97, 110)), (6, (32, 73)), (5, (115, 117)), (5, (112, 101)), (5, (111, 114)), (5, (111, 102)), (5, (108, 101)), (5, (101, 116)), (5, (101, 109)), (5, (100, 117)), (5, (32, 112)), (5, (32, 111)), (4, (121, 112)), (4, (118, 101)), (4, (116, 121)), (4, (116, 114)), (4, (116, 111)), (4, (116, 105)), (4, (114, 105)), (4, (112, 115)), (4, (111, 110)), (4, (108, 121)), (4, (102, 32)), (4, (101, 114)), (4, (101, 100)), (4, (76, 111)), (4, (73, 112)), (4, (44, 32)), (4, (32, 119)), (4, (32, 76)), (3, (117, 115)), (3, (116, 101)), (3, (115, 116)), (3, (115, 105)), (3, (115, 44)), (3, (114, 32)), (3, (111, 32)), (3, (110, 105)), (3, (110, 99)), (3, (107, 101)), (3, (105, 116)), (3, (105, 115)), (3, (104, 97)), (3, (103, 101)), (3, (101, 99)), (3, (99, 101)), (3, (97, 114)), (3, (97, 108)), (3, (46, 32)), (3, (32, 114)), (3, (32, 101)), (3, (32, 100)), (3, (32, 98)), (2, (120, 116)), (2, (119, 105)), (2, (119, 97)), (2, (117, 114)), (2, (117, 110)), (2, (116, 116)), (2, (116, 97)), (2, (115, 115)), (2, (115, 111)), (2, (115, 104)), (2, (114, 121)), (2, (114, 97)), (2, (112, 117)), (2, (112, 114)), (2, (112, 32)), (2, (111, 112)), (2, (111, 111)), (2, (111, 107)), (2, (110, 111)), (2, (109, 121)), (2, (109, 109)), (2, (109, 97)), (2, (108, 108)), (2, (108, 105)), (2, (105, 118)), (2, (105, 109)), (2, (104, 32)), (2, (101, 120)), (2, (101, 108)), (2, (101, 101)), (2, (101, 97)), (2, (98, 108)), (2, (97, 107)), (2, (97, 105)), (2, (97, 103)), (2, (97, 32)), (2, (73, 116)), (2, (48, 115)), (2, (32, 117)), (2, (32, 109)), (2, (32, 108)), (2, (32, 104)), (2, (32, 99)), (2, (32, 49)), (1, (121, 46)), (1, (121, 39)), (1, (119, 110)), (1, (119, 104)), (1, (118, 105)), (1, (117, 116)), (1, (117, 108)), (1, (117, 100)), (1, (117, 98)), (1, (116, 119)), (1, (116, 117)), (1, (116, 115)), (1, (116, 108)), (1, (115, 112)), (1, (115, 107)), (1, (115, 99)), (1, (115, 97)), (1, (114, 118)), (1, (114, 115)), (1, (114, 111)), (1, (114, 100)), (1, (112, 111)), (1, (112, 108)), (1, (112, 97)), (1, (111, 119)), (1, (111, 116)), (1, (110, 115)), (1, (110, 108)), (1, (110, 107)), (1, (109, 112)), (1, (109, 111)), (1, (109, 101)), (1, (109, 98)), (1, (109, 46)), (1, (108, 117)), (1, (108, 115)), (1, (108, 100)), (1, (108, 97)), (1, (107, 116)), (1, (107, 110)), (1, (107, 46)), (1, (107, 32)), (1, (105, 111)), (1, (105, 107)), (1, (105, 101)), (1, (105, 99)), (1, (105, 97)), (1, (104, 105)), (1, (103, 97)), (1, (103, 44)), (1, (102, 116)), (1, (102, 105)), (1, (101, 121)), (1, (101, 118)), (1, (101, 77)), (1, (100, 105)), (1, (100, 101)), (1, (100, 97)), (1, (100, 46)), (1, (99, 116)), (1, (99, 114)), (1, (99, 111)), (1, (99, 108)), (1, (99, 105)), (1, (99, 104)), (1, (99, 32)), (1, (98, 117)), (1, (98, 111)), (1, (98, 101)), (1, (97, 112)), (1, (97, 109)), (1, (80, 97)), (1, (77, 97)), (1, (76, 101)), (1, (65, 108)), (1, (57, 54)), (1, (54, 48)), (1, (53, 48)), (1, (49, 57)), (1, (49, 53)), (1, (48, 48)), (1, (39, 115)), (1, (32, 118)), (1, (32, 110)), (1, (32, 103)), (1, (32, 102)), (1, (32, 80)), (1, (32, 65))]\n",
            "(105, 110)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterative merging --> merging the top_pair\n",
        "\n",
        "def merge(ids, pair ,idx):\n",
        "  new_ids = []\n",
        "  i = 0\n",
        "\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)\n",
        "      i +=2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i +=1\n",
        "\n",
        "  return new_ids\n",
        "\n",
        "# print(merge([5,6,6,7,9,1] , (6,7) , 99))\n",
        "\n",
        "tokens2 = merge(tokens , top_pair , 256)\n",
        "print(tokens2 , f\"len : {len(tokens2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhAq_3TNJW6y",
        "outputId": "91ac847c-a11d-4601-ce52-741d7e791854"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 32, 105, 115, 32, 115, 105, 109, 112, 108, 121, 32, 100, 117, 109, 109, 121, 32, 116, 101, 120, 116, 32, 111, 102, 32, 116, 104, 101, 32, 112, 114, 256, 116, 256, 103, 32, 97, 110, 100, 32, 116, 121, 112, 101, 115, 101, 116, 116, 256, 103, 32, 256, 100, 117, 115, 116, 114, 121, 46, 32, 76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 32, 104, 97, 115, 32, 98, 101, 101, 110, 32, 116, 104, 101, 32, 256, 100, 117, 115, 116, 114, 121, 39, 115, 32, 115, 116, 97, 110, 100, 97, 114, 100, 32, 100, 117, 109, 109, 121, 32, 116, 101, 120, 116, 32, 101, 118, 101, 114, 32, 115, 256, 99, 101, 32, 116, 104, 101, 32, 49, 53, 48, 48, 115, 44, 32, 119, 104, 101, 110, 32, 97, 110, 32, 117, 110, 107, 110, 111, 119, 110, 32, 112, 114, 256, 116, 101, 114, 32, 116, 111, 111, 107, 32, 97, 32, 103, 97, 108, 108, 101, 121, 32, 111, 102, 32, 116, 121, 112, 101, 32, 97, 110, 100, 32, 115, 99, 114, 97, 109, 98, 108, 101, 100, 32, 105, 116, 32, 116, 111, 32, 109, 97, 107, 101, 32, 97, 32, 116, 121, 112, 101, 32, 115, 112, 101, 99, 105, 109, 101, 110, 32, 98, 111, 111, 107, 46, 32, 73, 116, 32, 104, 97, 115, 32, 115, 117, 114, 118, 105, 118, 101, 100, 32, 110, 111, 116, 32, 111, 110, 108, 121, 32, 102, 105, 118, 101, 32, 99, 101, 110, 116, 117, 114, 105, 101, 115, 44, 32, 98, 117, 116, 32, 97, 108, 115, 111, 32, 116, 104, 101, 32, 108, 101, 97, 112, 32, 256, 116, 111, 32, 101, 108, 101, 99, 116, 114, 111, 110, 105, 99, 32, 116, 121, 112, 101, 115, 101, 116, 116, 256, 103, 44, 32, 114, 101, 109, 97, 256, 256, 103, 32, 101, 115, 115, 101, 110, 116, 105, 97, 108, 108, 121, 32, 117, 110, 99, 104, 97, 110, 103, 101, 100, 46, 32, 73, 116, 32, 119, 97, 115, 32, 112, 111, 112, 117, 108, 97, 114, 105, 115, 101, 100, 32, 256, 32, 116, 104, 101, 32, 49, 57, 54, 48, 115, 32, 119, 105, 116, 104, 32, 116, 104, 101, 32, 114, 101, 108, 101, 97, 115, 101, 32, 111, 102, 32, 76, 101, 116, 114, 97, 115, 101, 116, 32, 115, 104, 101, 101, 116, 115, 32, 99, 111, 110, 116, 97, 256, 256, 103, 32, 76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 32, 112, 97, 115, 115, 97, 103, 101, 115, 44, 32, 97, 110, 100, 32, 109, 111, 114, 101, 32, 114, 101, 99, 101, 110, 116, 108, 121, 32, 119, 105, 116, 104, 32, 100, 101, 115, 107, 116, 111, 112, 32, 112, 117, 98, 108, 105, 115, 104, 256, 103, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 108, 105, 107, 101, 32, 65, 108, 100, 117, 115, 32, 80, 97, 103, 101, 77, 97, 107, 101, 114, 32, 256, 99, 108, 117, 100, 256, 103, 32, 118, 101, 114, 115, 105, 111, 110, 115, 32, 111, 102, 32, 76, 111, 114, 101, 109, 32, 73, 112, 115, 117, 109, 46] len : 557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it will iterate until the desired vocab size is reached\n",
        "\n",
        "vocab_size = 276 # the desired vocab size\n",
        "num_merges = vocab_size - 256\n",
        "lds = list(tokens) # copuy and do not destory the og\n",
        "\n",
        "merges = {} # (int , int) = int\n",
        "\n",
        "for i in range(num_merges):\n",
        "  stats = get_pair_freq(lds)\n",
        "  top_pair = max(stats , key = stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging : {top_pair} into a new token {idx}\")\n",
        "  merges[top_pair] = idx\n",
        "  lds = merge(lds , top_pair , idx)\n",
        "\n",
        "\n",
        "print(lds)\n",
        "print(len(lds))\n",
        "\n",
        "print(f\"vocab size = {len(set(lds))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-UDsK_GJ__K",
        "outputId": "d2343e1c-824f-4377-b1ba-af6703f685b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging : (105, 110) into a new token 256\n",
            "merging : (101, 32) into a new token 257\n",
            "merging : (32, 116) into a new token 258\n",
            "merging : (115, 32) into a new token 259\n",
            "merging : (114, 101) into a new token 260\n",
            "merging : (109, 32) into a new token 261\n",
            "merging : (116, 32) into a new token 262\n",
            "merging : (256, 103) into a new token 263\n",
            "merging : (104, 257) into a new token 264\n",
            "merging : (263, 32) into a new token 265\n",
            "merging : (97, 110) into a new token 266\n",
            "merging : (101, 115) into a new token 267\n",
            "merging : (101, 110) into a new token 268\n",
            "merging : (100, 32) into a new token 269\n",
            "merging : (115, 117) into a new token 270\n",
            "merging : (121, 32) into a new token 271\n",
            "merging : (100, 117) into a new token 272\n",
            "merging : (111, 102) into a new token 273\n",
            "merging : (258, 264) into a new token 274\n",
            "merging : (108, 101) into a new token 275\n",
            "[76, 111, 260, 261, 73, 112, 270, 261, 105, 259, 115, 105, 109, 112, 108, 271, 272, 109, 109, 121, 258, 101, 120, 262, 273, 274, 112, 114, 256, 116, 265, 266, 100, 258, 121, 112, 267, 101, 116, 116, 265, 256, 272, 115, 116, 114, 121, 46, 32, 76, 111, 260, 261, 73, 112, 270, 261, 104, 97, 259, 98, 101, 268, 274, 256, 272, 115, 116, 114, 121, 39, 259, 115, 116, 266, 100, 97, 114, 269, 272, 109, 109, 121, 258, 101, 120, 262, 101, 118, 101, 114, 32, 115, 256, 99, 257, 116, 264, 49, 53, 48, 48, 115, 44, 32, 119, 104, 268, 32, 266, 32, 117, 110, 107, 110, 111, 119, 110, 32, 112, 114, 256, 116, 101, 114, 258, 111, 111, 107, 32, 97, 32, 103, 97, 108, 275, 271, 273, 258, 121, 112, 257, 266, 269, 115, 99, 114, 97, 109, 98, 275, 269, 105, 116, 258, 111, 32, 109, 97, 107, 257, 97, 258, 121, 112, 257, 115, 112, 101, 99, 105, 109, 268, 32, 98, 111, 111, 107, 46, 32, 73, 262, 104, 97, 259, 270, 114, 118, 105, 118, 101, 269, 110, 111, 262, 111, 110, 108, 271, 102, 105, 118, 257, 99, 268, 116, 117, 114, 105, 267, 44, 32, 98, 117, 262, 97, 108, 115, 111, 274, 275, 97, 112, 32, 256, 116, 111, 32, 101, 275, 99, 116, 114, 111, 110, 105, 99, 258, 121, 112, 267, 101, 116, 116, 263, 44, 32, 260, 109, 97, 256, 265, 267, 115, 268, 116, 105, 97, 108, 108, 271, 117, 110, 99, 104, 266, 103, 101, 100, 46, 32, 73, 262, 119, 97, 259, 112, 111, 112, 117, 108, 97, 114, 105, 115, 101, 269, 256, 274, 49, 57, 54, 48, 259, 119, 105, 116, 104, 274, 260, 275, 97, 115, 257, 273, 32, 76, 101, 116, 114, 97, 115, 101, 262, 115, 104, 101, 101, 116, 259, 99, 111, 110, 116, 97, 256, 265, 76, 111, 260, 261, 73, 112, 270, 261, 112, 97, 115, 115, 97, 103, 267, 44, 32, 266, 269, 109, 111, 114, 257, 260, 99, 268, 116, 108, 271, 119, 105, 116, 104, 32, 100, 267, 107, 116, 111, 112, 32, 112, 117, 98, 108, 105, 115, 104, 265, 115, 273, 116, 119, 97, 114, 257, 108, 105, 107, 257, 65, 108, 272, 259, 80, 97, 103, 101, 77, 97, 107, 101, 114, 32, 256, 99, 108, 117, 100, 265, 118, 101, 114, 115, 105, 111, 110, 259, 273, 32, 76, 111, 260, 261, 73, 112, 270, 109, 46]\n",
            "426\n",
            "vocab size = 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"token list at start = {len(tokens)}\")\n",
        "print(f\" final len = {len(lds)}\")\n",
        "print(f\"compression ratio : {len(tokens)/len(lds)}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puy4Pu4wIfHW",
        "outputId": "df62901f-79f9-4290-a624-0b274e433f62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token list at start = 574\n",
            " final len = 426\n",
            "compression ratio : 1.3474178403755868X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text:str):\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while True:\n",
        "    stats = get_pair_freq(tokens)\n",
        "    if not stats:\n",
        "      break\n",
        "    pair = min(stats , key = lambda p: merges.get(p , float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens , pair , idx)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "print(encode(\"heloo world\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLIS_w5aUmQw",
        "outputId": "de599710-025d-4cd9-d69c-ded07b2874f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104, 101, 108, 111, 111, 32, 119, 111, 114, 108, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder (Tokenization):\n",
        "1. Lookup Table:\n",
        "The decoder uses the vocabulary created by the encoder as a lookup table.\n",
        "2. Subword Decomposition:\n",
        "When decoding a new piece of text, the decoder searches the vocabulary for the longest subword that matches the current prefix of the input text.\n",
        "3. Tokenization:\n",
        "The decoder then replaces the matched subword with the corresponding token from the vocabulary.\n",
        "4. Iteration:\n",
        "This process is repeated until the entire input text has been broken down into a sequence of tokens."
      ],
      "metadata": {
        "id": "akHjBOUAP9DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merges --> lookup table for decoding the subwords into the real worlds\n",
        "\n",
        "vocab = {idx : bytes([idx]) for idx in range(256)}\n",
        "for (p0 , p1) , idx in sorted(merges.items() , key= lambda x : x[1]):\n",
        "  vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "# print(list(vocab))\n",
        "\n",
        "def decode(lds):\n",
        "  \"given lds (list of integers) --> python string\"\n",
        "  tokens = b\"\".join(vocab[idx] for idx in lds)\n",
        "  text = tokens.decode(\"utf-8\" , errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "text2 = decode(lds)\n",
        "print(text2 == text)\n",
        "\n",
        "print(decode([256]))\n",
        "print(decode(encode(\"hello world\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un_jZsoMQBxn",
        "outputId": "cb0611d7-000b-40d5-e3c5-ef33142d7141"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "in\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "full working mode and summary --> remember the code is beutify by the chatgpt"
      ],
      "metadata": {
        "id": "JoZCEapLXt58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------\n",
        "# UTILITY: Find the most frequent adjacent byte pairs\n",
        "# ---------------------------------------\n",
        "def get_pair_freq(tokens):\n",
        "  pair_freq = {}\n",
        "  for pair in zip(tokens, tokens[1:]):\n",
        "    pair_freq[pair] = pair_freq.get(pair , 0) + 1\n",
        "  return pair_freq\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# UTILITY: Merge the most frequent pair into a new token ID\n",
        "# ---------------------------------------\n",
        "def merge(ids, pair, idx):\n",
        "  new_ids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      new_ids.append(idx)  # merge the pair into a new token\n",
        "      i += 2\n",
        "    else:\n",
        "      new_ids.append(ids[i])\n",
        "      i += 1\n",
        "  return new_ids\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ⬇️ TRAINING PHASE: Learn merges from initial text ⬇️\n",
        "# =====================================================\n",
        "\n",
        "vocab_size = 276                # desired vocabulary size\n",
        "num_merges = vocab_size - 256  # how many merges we will do\n",
        "lds = list(tokens)             # input byte token list (training data)\n",
        "\n",
        "merges = {}  # dictionary: (pair of ints) -> new token index\n",
        "\n",
        "# 🔁 Iteratively perform merges to build BPE vocabulary\n",
        "for i in range(num_merges):\n",
        "  stats = get_pair_freq(lds)\n",
        "  if not stats:\n",
        "    break\n",
        "  top_pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i  # new token index starts at 256\n",
        "  print(f\"merging : {top_pair} into a new token {idx}\")\n",
        "  merges[top_pair] = idx\n",
        "  lds = merge(lds, top_pair, idx)  # apply the merge\n",
        "\n",
        "# ==========================================================\n",
        "# ⬇️ VOCABULARY CONSTRUCTION: Map token IDs to byte strings ⬇️\n",
        "# ==========================================================\n",
        "\n",
        "# base vocab: 0–255 → single byte values\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "\n",
        "# apply merges in order to build up the new tokens\n",
        "for (p0, p1), idx in sorted(merges.items(), key=lambda x: x[1]):\n",
        "  vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "# =====================================================\n",
        "# ⬇️ ENCODING FUNCTION: Text → List of merged token IDs ⬇️\n",
        "# =====================================================\n",
        "def encode(text: str):\n",
        "  tokens = list(text.encode(\"utf-8\"))  # convert to byte list\n",
        "  while True:\n",
        "    stats = get_pair_freq(tokens)\n",
        "    if not stats:\n",
        "      break\n",
        "    # pick the most prioritary pair (lowest merge index)\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# ⬇️ DECODING FUNCTION: List of token IDs → String ⬇️\n",
        "# ====================================================\n",
        "def decode(lds):\n",
        "  tokens = b\"\".join(vocab[idx] for idx in lds)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "# ================================\n",
        "# ⬇️ Example Usage / Testing ⬇️\n",
        "# ================================\n",
        "print(encode(\"heloo world\"))   # encoded token ID list\n",
        "print(decode([104, 101, 108, 111, 111, 32, 119, 111, 114, 108, 100]))  # decoded from bytes\n",
        "print(decode(encode(\"hello world\")))  # full round-trip test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8n5Gqo5QXv_e",
        "outputId": "0d529628-03ee-4fd9-dfee-f9be19f9d814"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging : (105, 110) into a new token 256\n",
            "merging : (101, 32) into a new token 257\n",
            "merging : (32, 116) into a new token 258\n",
            "merging : (115, 32) into a new token 259\n",
            "merging : (114, 101) into a new token 260\n",
            "merging : (109, 32) into a new token 261\n",
            "merging : (116, 32) into a new token 262\n",
            "merging : (256, 103) into a new token 263\n",
            "merging : (104, 257) into a new token 264\n",
            "merging : (263, 32) into a new token 265\n",
            "merging : (97, 110) into a new token 266\n",
            "merging : (101, 115) into a new token 267\n",
            "merging : (101, 110) into a new token 268\n",
            "merging : (100, 32) into a new token 269\n",
            "merging : (115, 117) into a new token 270\n",
            "merging : (121, 32) into a new token 271\n",
            "merging : (100, 117) into a new token 272\n",
            "merging : (111, 102) into a new token 273\n",
            "merging : (258, 264) into a new token 274\n",
            "merging : (108, 101) into a new token 275\n",
            "[104, 101, 108, 111, 111, 32, 119, 111, 114, 108, 100]\n",
            "heloo world\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forced splits using regex pattern"
      ],
      "metadata": {
        "id": "Y6hxpu9_d19N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\" , flags=re.IGNORECASE)\n",
        "\n",
        "print(re.findall(gpt2pat , \"Hello World HOW'S are YOU 2342            \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IElwKNhZd85",
        "outputId": "a5759956-bf83-43ad-e0a9-76dae3bb56b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ' World', ' HOW', \"'S\", ' are', ' YOU', ' 2342', '            ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eg = \"\"\"for i in range(1, 6):\n",
        "  for j in range(i):\n",
        "    print(\"*\", end=\"\")\n",
        "  print()\"\"\"\n",
        "\n",
        "print(re.findall(gpt2pat , eg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC6v2v58NpNk",
        "outputId": "50d3446e-76be-4a05-cec2-e5bb87ccab8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', ' i', ' in', ' range', '(', '1', ',', ' 6', '):', '\\n ', ' for', ' j', ' in', ' range', '(', 'i', '):', '\\n   ', ' print', '(\"*\",', ' end', '=\"\")', '\\n ', ' print', '()']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the GPT-4 Tokenizer\n",
        "\n",
        "https://github.com/karpathy/minbpe/blob/master/exercise.md"
      ],
      "metadata": {
        "id": "qNRPjwnjmEe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/karpathy/minbpe/refs/heads/master/tests/taylorswift.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF3CT0P1Gu_X",
        "outputId": "1d2c5c93-352c-4b3d-8e8c-17ff62d45ddb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-12 14:35:38--  https://raw.githubusercontent.com/karpathy/minbpe/refs/heads/master/tests/taylorswift.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185768 (181K) [text/plain]\n",
            "Saving to: ‘taylorswift.txt’\n",
            "\n",
            "taylorswift.txt     100%[===================>] 181.41K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-06-12 14:35:38 (8.05 MB/s) - ‘taylorswift.txt’ saved [185768/185768]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1\n",
        "\n",
        "class BasicTokenizer():\n",
        "  def __init__(self  , special_tokens:list = None) -> None:\n",
        "    self.vocab_size = 0\n",
        "    self.verbose = False\n",
        "    self.special_tokens = special_tokens or []\n",
        "    self.merges = {} # --> {(int , int) : int}\n",
        "    self.vocab = {} # --> {token_str : token_id}\n",
        "    self.vocab_inv = {} # --> {token_id : token_str}\n",
        "    # GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
        "    self.patterns = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\" , flags=re.IGNORECASE)\n",
        "\n",
        "  def get_pair_freq(self, tokens):\n",
        "    pairs = {}\n",
        "    for pair in zip(tokens , tokens[1:]):\n",
        "      pairs[pair] = pairs.get(pair , 0) + 1\n",
        "\n",
        "    return pairs\n",
        "\n",
        "  def merge(self , ids, pair, idx):\n",
        "    new_ids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "      if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "        new_ids.append(idx)  # merge the pair into a new token\n",
        "        i += 2\n",
        "      else:\n",
        "        new_ids.append(ids[i])\n",
        "        i += 1\n",
        "    return new_ids\n",
        "\n",
        "  def train(self , text:str , vocab_size:int, verbose:bool = False):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.verbose = verbose\n",
        "\n",
        "    words = re.findall(self.patterns , text)\n",
        "    tokens = [c for word in words for c in list(word) + [\"<W>\"]]\n",
        "    tokens = list(\"\".join(tokens).encode(\"utf-8\"))\n",
        "\n",
        "    next_idx = 256\n",
        "\n",
        "    self.vocab = {idx : bytes([idx]) for idx in range(next_idx)}\n",
        "\n",
        "    tokens_copy = list(tokens)\n",
        "    # finding the most frequent pairs\n",
        "    num_merges = vocab_size - 256\n",
        "    for i in range(num_merges):\n",
        "      stats = self.get_pair_freq(tokens)\n",
        "      if not stats:\n",
        "        break\n",
        "      max_freq_pair = max(stats , key=stats.get)\n",
        "      next_idx +=  1\n",
        "      if self.verbose:\n",
        "        print(f\"merging : {max_freq_pair} --> {next_idx}\")\n",
        "\n",
        "      self.merges[max_freq_pair] = next_idx\n",
        "      self.vocab[next_idx] = self.vocab[max_freq_pair[0]] + self.vocab[max_freq_pair[1]]\n",
        "      tokens = self.merge(tokens , max_freq_pair , next_idx)\n",
        "\n",
        "    self.vocab_inv = {idx : self.vocab[idx] for idx in self.vocab}\n",
        "\n",
        "  def encode(self , text:str):\n",
        "      words = re.findall(self.patterns , text)\n",
        "      tokens = [c for word in words for c in list(word)]\n",
        "      tokens = list(\"\".join(tokens).encode(\"utf-8\"))\n",
        "      while True:\n",
        "        stats = self.get_pair_freq(tokens)\n",
        "        if not stats:\n",
        "          break\n",
        "        # pick the most prioritary pair (lowest merge index)\n",
        "        pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
        "        if pair not in self.merges:\n",
        "          break\n",
        "        idx = self.merges[pair]\n",
        "        tokens = self.merge(tokens, pair, idx)\n",
        "      return tokens\n",
        "\n",
        "  def decode(self,lds):\n",
        "      tokens = b\"\".join(self.vocab[idx] for idx in lds)\n",
        "      text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "      return text\n"
      ],
      "metadata": {
        "id": "ulyc8MD-miiE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BasicTokenizer()\n",
        "with open(\"taylorswift.txt\" , \"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "# tokenizer.train(\"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\"\"\" , 50257)\n",
        "tokenizer.train(text , 50257)\n",
        "\n",
        "tokens = tokenizer.encode(\"hello world!!!? (안녕하세요!) lol123 😉\")\n",
        "print(tokens)\n",
        "\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dckXYVEttQzT",
        "outputId": "375f863d-3c86-4d6b-dea7-389394e6fdd4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[104, 665, 111, 32, 4144, 2801, 102, 32, 2626, 341, 116, 32, 523, 32, 609, 1897, 32, 3843, 121, 267, 32, 6503, 39, 115, 32, 3237]\n",
            "hello myself Rachit and like Talyor Swift's songs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match this\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"hello world!!!? (안녕하세요!) lol123 😉\")\n",
        "text = enc.decode(ids) # get the same text back\n",
        "\n",
        "\n",
        "tokens = tokenizer.encode(\"hello world!!!? (안녕하세요!) lol123 😉\")\n",
        "# print(tokens)\n",
        "\n",
        "decoded_text = tokenizer.decode(tokens)\n",
        "print(decoded_text == text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CirL1S-akTXN",
        "outputId": "f4ef2161-508b-4ac3-d982-528702e636f0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}