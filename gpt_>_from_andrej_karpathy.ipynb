{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZVfeAcjECjJRT5Do/65VY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/Large-Language-Model/blob/main/gpt_%3E_from_andrej_karpathy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brK_uV_z3ON2",
        "outputId": "7e5cf770-52a8-4d7f-eeb8-a3b1c6384544"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-11 01:41:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.4’\n",
            "\n",
            "input.txt.4         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-06-11 01:41:19 (26.6 MB/s) - ‘input.txt.4’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q4lpZf0ezh_",
        "outputId": "95513bd0-99e0-478c-f299-7240cd1535f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url= \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "resp = requests.get(url)\n",
        "\n",
        "text = resp.text\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErwB7VkrzGHc",
        "outputId": "b3236bc2-8f7e-4bf6-9485-4fbd47e32ffa"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize"
      ],
      "metadata": {
        "id": "PQ0x2ERD0zO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building the character level tokenizer\n",
        "stoi = {ch:i for i ,ch in enumerate(chars)}\n",
        "itos = {value:key for key , value in stoi.items()}\n",
        "\n",
        "encoder = lambda s: [stoi[c] for c in s]\n",
        "decoder = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "print(stoi)\n",
        "print(itos)\n",
        "\n",
        "print(encoder(\"hii there\"))\n",
        "print(decoder(encoder(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6lBH52Y0hql",
        "outputId": "06c8d7d8-a620-4964-b6be-2eb0e2a0e4bc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tiktoken --> use by openai's models"
      ],
      "metadata": {
        "id": "KRnJJ-2l2mWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "enc.n_vocab\n",
        "\n",
        "print(enc.encode(\"hii there\"))\n",
        "print(enc.decode(enc.encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFW1x5i21HeK",
        "outputId": "de89a5a3-14c7-4804-cd5f-da891701283b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71, 4178, 612]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using sentencepiece tokenizer\n",
        "\n",
        "! pip install sentencepiece\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(input=\"input.txt\", model_prefix=\"m\", vocab_size=5000)\n",
        "\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Load model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# Encode text into subword tokens\n",
        "tokens = sp.encode(\"This is an example\", out_type=str)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Encode as IDs\n",
        "ids = sp.encode(\"This is an example\", out_type=int)\n",
        "print(\"IDs:\", ids)\n",
        "\n",
        "# Decode back to string\n",
        "decoded = sp.decode(ids)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZ4v4mbj3BNu",
        "outputId": "4d008678-9859-46b6-8d06-e46155b544cf"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Tokens: ['▁This', '▁is', '▁an', '▁example']\n",
            "IDs: [191, 25, 135, 3016]\n",
            "Decoded: This is an example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DO NOT CLOSE THIS"
      ],
      "metadata": {
        "id": "a1cJT-Sj38HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# encoding the whole data into tensor\n",
        "\n",
        "data = torch.tensor(encoder(text) , dtype=torch.long)\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIJ2VobA2iyJ",
        "outputId": "dcc64c48-6c24-4448-b21d-82907f5a0be6"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitint the data into test and train\n",
        "\n",
        "n = 0.9*len(data)\n",
        "\n",
        "train_data = data[:int(n)]\n",
        "val_data = data[int(n):]\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVKAixzN4J0-",
        "outputId": "b764d115-5102-4bb4-cad2-04cbc1b446e7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1003854])\n",
            "torch.Size([111540])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader , Dataset\n",
        "\n",
        "BLOCK_SIZE = 8\n",
        "# train_data[:BLOCK_SIZE]\n",
        "\n",
        "# how we going to provide the data in the transformer\n",
        "x = train_data[:BLOCK_SIZE]\n",
        "y = train_data[1:BLOCK_SIZE+1]\n",
        "\n",
        "pad = [0]\n",
        "\n",
        "for t in range(BLOCK_SIZE):\n",
        "  context = torch.cat((x[:t+1] , torch.tensor(pad*(BLOCK_SIZE - t))))\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} , then target is {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YxxRYGk4nHq",
        "outputId": "07f0ad94-fa3b-4844-9550-3b26d5ec5fa8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18,  0,  0,  0,  0,  0,  0,  0,  0]) , then target is 47\n",
            "when input is tensor([18, 47,  0,  0,  0,  0,  0,  0,  0]) , then target is 56\n",
            "when input is tensor([18, 47, 56,  0,  0,  0,  0,  0,  0]) , then target is 57\n",
            "when input is tensor([18, 47, 56, 57,  0,  0,  0,  0,  0]) , then target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58,  0,  0,  0,  0]) , then target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1,  0,  0,  0]) , then target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15,  0,  0]) , then target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47,  0]) , then target is 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78eeLjHwoFbp",
        "outputId": "b466db5a-8b9e-4940-99c3-2b41e35d0174"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Str\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "def get_batch(split:Str , batch_size=BATCH_SIZE):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "\n",
        "  ix = torch.randint(0 , len(data)-BLOCK_SIZE, (batch_size,))\n",
        "\n",
        "  x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+BLOCK_SIZE + 1] for i in ix])\n",
        "\n",
        "  return x , y\n",
        "\n",
        "xb, yb = get_batch('train' , BATCH_SIZE)\n",
        "xb = xb.to(device)\n",
        "yb = yb.to(device)\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "for b in range(BATCH_SIZE):\n",
        "  for t in range(BLOCK_SIZE):\n",
        "    context = torch.cat((xb[b, :t+1] , torch.tensor(pad*(BLOCK_SIZE - t+1)).to(device)))\n",
        "    target = yb[b , t]\n",
        "\n",
        "    print(f\"when input is {context} , then target is {target}\")\n",
        "  break # i will see only one batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7jh3Z-_5i8m",
        "outputId": "1e33d964-f8ed-46ce-d8c2-9e8af0931710"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8])\n",
            "tensor([[ 5, 42,  1, 48, 53, 47, 52, 58],\n",
            "        [ 0,  0, 15, 50, 53, 61, 52, 10],\n",
            "        [63, 39, 50,  1, 57, 47, 56,  6],\n",
            "        [25, 26, 21, 13, 10,  0, 27,  6]], device='cuda:0')\n",
            "torch.Size([4, 8])\n",
            "tensor([[42,  1, 48, 53, 47, 52, 58,  1],\n",
            "        [ 0, 15, 50, 53, 61, 52, 10,  0],\n",
            "        [39, 50,  1, 57, 47, 56,  6,  1],\n",
            "        [26, 21, 13, 10,  0, 27,  6,  1]], device='cuda:0')\n",
            "when input is tensor([5, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0') , then target is 42\n",
            "when input is tensor([ 5, 42,  0,  0,  0,  0,  0,  0,  0,  0], device='cuda:0') , then target is 1\n",
            "when input is tensor([ 5, 42,  1,  0,  0,  0,  0,  0,  0,  0], device='cuda:0') , then target is 48\n",
            "when input is tensor([ 5, 42,  1, 48,  0,  0,  0,  0,  0,  0], device='cuda:0') , then target is 53\n",
            "when input is tensor([ 5, 42,  1, 48, 53,  0,  0,  0,  0,  0], device='cuda:0') , then target is 47\n",
            "when input is tensor([ 5, 42,  1, 48, 53, 47,  0,  0,  0,  0], device='cuda:0') , then target is 52\n",
            "when input is tensor([ 5, 42,  1, 48, 53, 47, 52,  0,  0,  0], device='cuda:0') , then target is 58\n",
            "when input is tensor([ 5, 42,  1, 48, 53, 47, 52, 58,  0,  0], device='cuda:0') , then target is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self - Attention\n",
        "\n",
        "d_k --> no of heads\n",
        "\n",
        "# attention = softmax((q @ transpose of k) / sqrt(d_k)) @ v --> acc to paper"
      ],
      "metadata": {
        "id": "GMGamjh4IGbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 32\n",
        "BLOCK_SIZE = 8\n",
        "BATCH_SIZE = 4"
      ],
      "metadata": {
        "id": "T5djGL3ofNJE"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"single head of self-attention\"\n",
        "\n",
        "  def __init__(self , head_size):\n",
        "    super().__init__()\n",
        "    self.head_size = head_size\n",
        "\n",
        "    self.key = nn.Linear(emb_dim , head_size , bias=False)\n",
        "    self.query = nn.Linear(emb_dim , head_size , bias=False)\n",
        "    self.value = nn.Linear(emb_dim , head_size , bias=False)\n",
        "    self.register_buffer('tril' , torch.tril(torch.ones(BLOCK_SIZE , BLOCK_SIZE)))\n",
        "\n",
        "  def forward(self , x):\n",
        "      B,T,C = x.shape\n",
        "      k = self.key(x) # --> [B,T,C]\n",
        "      q = self.query(x) # --> [B,T,C]\n",
        "      v = self.value(x) # --> [B,T,C]\n",
        "\n",
        "      attention_ = q@k.transpose(1,2)/ math.sqrt(C) # --> [B,T,T]\n",
        "      attention_ = attention_.masked_fill(self.tril == 0 , float('-inf'))\n",
        "      attention_ = F.softmax(attention_ , dim=-1)\n",
        "\n",
        "      return attention_ @ v # --> [B,T,T] @ [B,T,C] --> [B,T,C]"
      ],
      "metadata": {
        "id": "1Lt-GFt5cIC0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-head-attention"
      ],
      "metadata": {
        "id": "DOkwpL9khn20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self , num_heads , head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(emb_dim , emb_dim)\n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "  def forward(self , x):\n",
        "    out = torch.cat([h(x) for h in self.heads] , dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "JlUF8q4Shp_h"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward"
      ],
      "metadata": {
        "id": "LOhXzOQRjegk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Linear(emb_dim , 2048),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(2048 , emb_dim)\n",
        "    )\n",
        "  def forward(self , x):\n",
        "    return self.feed_forward(x)"
      ],
      "metadata": {
        "id": "wqcQh6qqjgd2"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "residual connection\n"
      ],
      "metadata": {
        "id": "9VEis9L8ijvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self , emb_dim , n_head):\n",
        "    super().__init__()\n",
        "    self.head_size = emb_dim // n_head\n",
        "    self.multi_head_attention = MultiHeadAttention(n_head , self.head_size)\n",
        "    self.feed_forward = FeedForward()\n",
        "    self.layer_norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "  def forward(self , x):\n",
        "    x = self.layer_norm(x +  self.multi_head_attention(x))\n",
        "    x = self.layer_norm(x +  self.feed_forward(x))\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "hFuj4GZsillL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LayerNorm --> in this we normalise the rows instead of columns like in BatchNorm"
      ],
      "metadata": {
        "id": "DCEQR64smIss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BilingualLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size , emb_dim)\n",
        "    self.position_embedding_table = nn.Embedding(BLOCK_SIZE , emb_dim)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(emb_dim , 4),\n",
        "        Block(emb_dim , 4),\n",
        "        Block(emb_dim , 4),\n",
        "        nn.LayerNorm(emb_dim)\n",
        "    )\n",
        "\n",
        "    # self.self_attention_head = MultiHeadAttention(4 , emb_dim//4) # --> 4 heads of 8 dimensional self-attention --> it will concat to give us the 32 dimensional vector --> [4,8,32]\n",
        "    # self.feed_forward = FeedForward()\n",
        "    self.lm_head = nn.Linear(emb_dim , vocab_size)\n",
        "\n",
        "  def forward(self , idx , target):\n",
        "    # x --> [batch , block] --> [batch , block , embedding_dim] (B,T,emb_dim) --> [batch , block , vocab_size]\n",
        "\n",
        "    '''tok_emb are the token embeddings for the input indices.\n",
        "      pos_emb is generated for positions 0 to T-1.'''\n",
        "\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.embedding(idx) # --> [B,T,C] --> [4,8,32]\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T , device=device)) # --> [T,C] --> [8,32]\n",
        "    x = tok_emb + pos_emb # --> [4 , 8 , 32]\n",
        "    # print(x.shape)\n",
        "    # x = self.self_attention_head(x) # --> [4 , 8 , 32]\n",
        "    # x = self.feed_forward(x) # --> [4,8,32]\n",
        "    x = self.blocks(x)\n",
        "    logits = self.lm_head(x) # --> [B,T,vocab_size] --> [4 , 8,65]\n",
        "\n",
        "    if target is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T , C) # --> [32 , 65]\n",
        "      target = target.view(-1)      # B*T\n",
        "      loss = F.cross_entropy(logits , target)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "  def generate(self ,idx , max_new_tokens):\n",
        "    # idx is the (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_condensed = idx[:, -BLOCK_SIZE:] # crop context to BLOCK_SIZE\n",
        "\n",
        "      logits , loss = self(idx_condensed , None)\n",
        "\n",
        "      logits = logits[: , -1 , :] # [batch ,vocab_size=65] --> selecting the last element of the batch in the time step\n",
        "      probs = F.softmax(logits , dim=-1)\n",
        "      idx_next = torch.multinomial(probs , num_samples=1) # --> (B,1)\n",
        "\n",
        "      # append sampled index to the running sequences\n",
        "      idx = torch.cat((idx , idx_next) , dim=1) # --> (B,T+1)...to (B,T+max_new_tokens)\n",
        "\n",
        "    return idx\n",
        "\n",
        "model = BilingualLanguageModel(vocab_size).to(device)\n",
        "logits , loss = model(xb , yb)\n",
        "\n",
        "print(logits.shape , loss)\n",
        "\n",
        "\n",
        "print(model.generate(xb , 5))\n",
        "\n",
        "# decoding the generated output\n",
        "print(decoder(model.generate(xb , 100)[3].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pOB8wXVh7tQc",
        "outputId": "ea10e76d-6f35-4b36-ce0f-6393c1795f3d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65]) tensor(4.3028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[ 5, 42,  1, 48, 53, 47, 52, 58, 13, 56, 51, 35,  3],\n",
            "        [ 0,  0, 15, 50, 53, 61, 52, 10,  1, 60, 52, 38, 42],\n",
            "        [63, 39, 50,  1, 57, 47, 56,  6, 10, 42,  9,  9, 23],\n",
            "        [25, 26, 21, 13, 10,  0, 27,  6, 41, 32, 20, 28, 26]], device='cuda:0')\n",
            "MNIA:\n",
            "O,bsysjHHCFAia Nerl?PEb-v3IAOvVTDq'TldsD  oeqVTvxK; k!\n",
            "?cAKMmwH3WJTw&:PNVj3eilP\n",
            "nwB:xb ud&oMWvqeiWWqKG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 200\n",
        "max_iters = 30000\n",
        "eval_interval = 300\n",
        "\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for split in ['train' , 'val']:\n",
        "      losses = torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "        X , Y = get_batch(split)\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "        logits , loss = model(X , Y)\n",
        "        losses[k] = loss.item()\n",
        "      out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "OM2feim2F527"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters() , lr=1e-3)\n",
        "model = model.to(device)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  if iter%eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step: {iter} || train_loss: {losses['train']:.2f} || val_loss: {losses['val']:.2f}\")\n",
        "  xb , yb = get_batch(\"train\" , 32)\n",
        "  xb = xb.to(device)\n",
        "  yb = yb.to(device)\n",
        "\n",
        "  logits , loss = model(xb , yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Z5moRZW9mgg",
        "outputId": "eb1520d5-b752-43e6-f15b-b94d281064b4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 || train_loss: 4.39 || val_loss: 4.38\n",
            "step: 300 || train_loss: 2.47 || val_loss: 2.49\n",
            "step: 600 || train_loss: 2.34 || val_loss: 2.36\n",
            "step: 900 || train_loss: 2.30 || val_loss: 2.27\n",
            "step: 1200 || train_loss: 2.16 || val_loss: 2.20\n",
            "step: 1500 || train_loss: 2.12 || val_loss: 2.16\n",
            "step: 1800 || train_loss: 2.10 || val_loss: 2.12\n",
            "step: 2100 || train_loss: 2.07 || val_loss: 2.12\n",
            "step: 2400 || train_loss: 2.12 || val_loss: 2.08\n",
            "step: 2700 || train_loss: 2.03 || val_loss: 2.07\n",
            "step: 3000 || train_loss: 2.00 || val_loss: 2.08\n",
            "step: 3300 || train_loss: 2.00 || val_loss: 2.05\n",
            "step: 3600 || train_loss: 1.96 || val_loss: 2.07\n",
            "step: 3900 || train_loss: 1.95 || val_loss: 2.06\n",
            "step: 4200 || train_loss: 1.93 || val_loss: 2.05\n",
            "step: 4500 || train_loss: 1.94 || val_loss: 2.03\n",
            "step: 4800 || train_loss: 1.94 || val_loss: 2.04\n",
            "step: 5100 || train_loss: 1.89 || val_loss: 2.00\n",
            "step: 5400 || train_loss: 1.90 || val_loss: 2.02\n",
            "step: 5700 || train_loss: 1.87 || val_loss: 1.99\n",
            "step: 6000 || train_loss: 1.90 || val_loss: 2.04\n",
            "step: 6300 || train_loss: 1.89 || val_loss: 2.01\n",
            "step: 6600 || train_loss: 1.87 || val_loss: 2.00\n",
            "step: 6900 || train_loss: 1.85 || val_loss: 2.00\n",
            "step: 7200 || train_loss: 1.84 || val_loss: 2.01\n",
            "step: 7500 || train_loss: 1.88 || val_loss: 2.00\n",
            "step: 7800 || train_loss: 1.84 || val_loss: 1.99\n",
            "step: 8100 || train_loss: 1.82 || val_loss: 2.00\n",
            "step: 8400 || train_loss: 1.86 || val_loss: 1.97\n",
            "step: 8700 || train_loss: 1.83 || val_loss: 2.01\n",
            "step: 9000 || train_loss: 1.86 || val_loss: 2.00\n",
            "step: 9300 || train_loss: 1.82 || val_loss: 1.98\n",
            "step: 9600 || train_loss: 1.80 || val_loss: 1.96\n",
            "step: 9900 || train_loss: 1.82 || val_loss: 1.95\n",
            "step: 10200 || train_loss: 1.84 || val_loss: 1.95\n",
            "step: 10500 || train_loss: 1.82 || val_loss: 1.99\n",
            "step: 10800 || train_loss: 1.81 || val_loss: 1.94\n",
            "step: 11100 || train_loss: 1.77 || val_loss: 1.98\n",
            "step: 11400 || train_loss: 1.79 || val_loss: 1.91\n",
            "step: 11700 || train_loss: 1.78 || val_loss: 1.95\n",
            "step: 12000 || train_loss: 1.80 || val_loss: 1.92\n",
            "step: 12300 || train_loss: 1.79 || val_loss: 1.93\n",
            "step: 12600 || train_loss: 1.81 || val_loss: 1.97\n",
            "step: 12900 || train_loss: 1.78 || val_loss: 1.95\n",
            "step: 13200 || train_loss: 1.77 || val_loss: 1.91\n",
            "step: 13500 || train_loss: 1.78 || val_loss: 1.93\n",
            "step: 13800 || train_loss: 1.76 || val_loss: 1.94\n",
            "step: 14100 || train_loss: 1.79 || val_loss: 1.91\n",
            "step: 14400 || train_loss: 1.75 || val_loss: 1.97\n",
            "step: 14700 || train_loss: 1.79 || val_loss: 1.91\n",
            "step: 15000 || train_loss: 1.77 || val_loss: 1.91\n",
            "step: 15300 || train_loss: 1.77 || val_loss: 1.93\n",
            "step: 15600 || train_loss: 1.75 || val_loss: 1.91\n",
            "step: 15900 || train_loss: 1.77 || val_loss: 1.88\n",
            "step: 16200 || train_loss: 1.79 || val_loss: 1.89\n",
            "step: 16500 || train_loss: 1.74 || val_loss: 1.90\n",
            "step: 16800 || train_loss: 1.77 || val_loss: 1.90\n",
            "step: 17100 || train_loss: 1.74 || val_loss: 1.95\n",
            "step: 17400 || train_loss: 1.74 || val_loss: 1.88\n",
            "step: 17700 || train_loss: 1.78 || val_loss: 1.88\n",
            "step: 18000 || train_loss: 1.76 || val_loss: 1.94\n",
            "step: 18300 || train_loss: 1.79 || val_loss: 1.88\n",
            "step: 18600 || train_loss: 1.74 || val_loss: 1.94\n",
            "step: 18900 || train_loss: 1.78 || val_loss: 1.87\n",
            "step: 19200 || train_loss: 1.74 || val_loss: 1.87\n",
            "step: 19500 || train_loss: 1.75 || val_loss: 1.92\n",
            "step: 19800 || train_loss: 1.73 || val_loss: 1.89\n",
            "step: 20100 || train_loss: 1.76 || val_loss: 1.87\n",
            "step: 20400 || train_loss: 1.76 || val_loss: 1.92\n",
            "step: 20700 || train_loss: 1.73 || val_loss: 1.88\n",
            "step: 21000 || train_loss: 1.76 || val_loss: 1.91\n",
            "step: 21300 || train_loss: 1.77 || val_loss: 1.92\n",
            "step: 21600 || train_loss: 1.76 || val_loss: 1.88\n",
            "step: 21900 || train_loss: 1.72 || val_loss: 1.91\n",
            "step: 22200 || train_loss: 1.76 || val_loss: 1.88\n",
            "step: 22500 || train_loss: 1.75 || val_loss: 1.87\n",
            "step: 22800 || train_loss: 1.75 || val_loss: 1.87\n",
            "step: 23100 || train_loss: 1.72 || val_loss: 1.88\n",
            "step: 23400 || train_loss: 1.72 || val_loss: 1.88\n",
            "step: 23700 || train_loss: 1.73 || val_loss: 1.86\n",
            "step: 24000 || train_loss: 1.72 || val_loss: 1.88\n",
            "step: 24300 || train_loss: 1.73 || val_loss: 1.87\n",
            "step: 24600 || train_loss: 1.73 || val_loss: 1.85\n",
            "step: 24900 || train_loss: 1.75 || val_loss: 1.89\n",
            "step: 25200 || train_loss: 1.73 || val_loss: 1.87\n",
            "step: 25500 || train_loss: 1.74 || val_loss: 1.89\n",
            "step: 25800 || train_loss: 1.70 || val_loss: 1.88\n",
            "step: 26100 || train_loss: 1.72 || val_loss: 1.89\n",
            "step: 26400 || train_loss: 1.76 || val_loss: 1.87\n",
            "step: 26700 || train_loss: 1.70 || val_loss: 1.89\n",
            "step: 27000 || train_loss: 1.75 || val_loss: 1.88\n",
            "step: 27300 || train_loss: 1.71 || val_loss: 1.87\n",
            "step: 27600 || train_loss: 1.72 || val_loss: 1.86\n",
            "step: 27900 || train_loss: 1.68 || val_loss: 1.88\n",
            "step: 28200 || train_loss: 1.74 || val_loss: 1.92\n",
            "step: 28500 || train_loss: 1.73 || val_loss: 1.91\n",
            "step: 28800 || train_loss: 1.71 || val_loss: 1.88\n",
            "step: 29100 || train_loss: 1.75 || val_loss: 1.89\n",
            "step: 29400 || train_loss: 1.72 || val_loss: 1.85\n",
            "step: 29700 || train_loss: 1.73 || val_loss: 1.84\n",
            "1.7447696924209595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder(model.generate(xb , 1000)[3].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNZtp-nSDvM0",
        "outputId": "7c2ce6c6-f0db-413b-a88c-4012215eef5d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arry, good\n",
            "Mellor: you not slear\n",
            "I amonoughters,\n",
            "Whost come, and we warrely me tire, plage thands; what that make far or laters me with u'er\n",
            "Most thee wars to bloward\n",
            "Marcum'tings with your furror lad stand to curse:\n",
            "These to prain himset uprict must or an queen.\n",
            "\n",
            "Second Selms of your eye,\n",
            "Only their gly the chories that thou old delam, how chold'dine land-beman:\n",
            "What not reprowt discourt. O friend Yox: with thank take for I immost begoa it, o'e through!\n",
            "Lettingers: crowar he did neven king, here earerquars the goad.\n",
            "\n",
            "GLOUCESTER:\n",
            "A knows come kines the invow? O let to joughtones on\n",
            "prevish, in thee mean?\n",
            "\n",
            "AUFIDIUS:\n",
            "I, yea:\n",
            "But is to-not man? I in followed very such advintedy to ceret om them lasts murder possin;\n",
            "Four earward for princenion.\n",
            "A trouble.\n",
            "\n",
            "CAMILLO:\n",
            "I dear about come crown oftlukes.\n",
            "Then? ind,\n",
            "First Citizes bring.\n",
            "No, true have to a migs his wort answer.\n",
            "\n",
            "BRUTUS:\n",
            "\n",
            "Serving, how sine ting sound me agged.\n",
            "'Sest no makes and bury could very flict.\n",
            "\n",
            "CLAUS:\n",
            "I shall abselvings of held.\n",
            "\n",
            "P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "x = torch.randn(4,8,32)\n",
        "\n",
        "head_size = 16 # --> N\n",
        "query = nn.Linear(emb_dim , head_size , bias=False)\n",
        "key = nn.Linear(emb_dim , head_size ,bias=False)\n",
        "value = nn.Linear(emb_dim , head_size ,bias=False)\n",
        "\n",
        "q = query(x) # --> [32 , 8,head_size]\n",
        "k = key(x) # --> [32,8,head_size]\n",
        "v = value(x) # --> [32,8,head_size]\n",
        "\n",
        "wei = q@k.transpose(1,2)/ math.sqrt(head_size) # --> ([32 , 8 , head_size] @ [32 , head_size , 8])@ [32 , 8 , head_size] --> [32,8,8] @ [32,8 , head_size] --> [32 , 8 , head_size]\n",
        "# print(wei[0])\n",
        "# now we will mask so that the model cannot see the future tokens\n",
        "\n",
        "tril = torch.tril(torch.ones(x.shape[1] , x.shape[1]))\n",
        "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
        "wei = F.softmax(wei , dim = -1)\n",
        "\n",
        "print(wei[0])\n",
        "\n",
        "xbow = wei @ v # --> [32,8,head_size] @ [32,8,65] --> [32,8,65]\n",
        "print(xbow.shape) # --> [32 , 8 , 65]\n",
        "# print(xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKq59OCT7RRY",
        "outputId": "f1e6a7a0-4986-4fe8-f812-a1bdc0207aa6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.6519, 0.3481, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2026, 0.2550, 0.5423, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2110, 0.2471, 0.2248, 0.3171, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1807, 0.2425, 0.1828, 0.2542, 0.1398, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1555, 0.1196, 0.0892, 0.2331, 0.1782, 0.2245, 0.0000, 0.0000],\n",
            "        [0.1802, 0.1169, 0.1308, 0.1486, 0.1730, 0.1471, 0.1035, 0.0000],\n",
            "        [0.1475, 0.1262, 0.0941, 0.1314, 0.0836, 0.1671, 0.1138, 0.1362]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HzpPMIfbLdgA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKBZbDTiy1J8"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}